# Flood Risk Management in the IJssel River
## Decision-Making under Deep Uncertainty through Modeling

Prepared for EPA141A by Group 10. 

|    Members             | Student Number |
| ---------------------- | -------------- |
| Nelene Augustinus      |   5404916      |
| Floor Broekman         |   5333806      |
| Nikhil Daswani         |   5049954      |
| Milan Moleman          |   5415764      |


## Contents
- [Flood Risk Management in the IJssel River](#flood-risk-management-in-the-ijssel-river)
  - [Decision-Making under Deep Uncertainty through Modeling](#decision-making-under-deep-uncertainty-through-modeling)
  - [Table of Contents](#table-of-contents)
  - [File Structure](#file-structure)
    - [Directories](#directories)
    - [Model \& Workbench Files](#model--workbench-files)
    - [Experimentation \& Analysis Files (\& Usage)](#experimentation--analysis-files--usage)
    - [Other Files](#other-files)
  - [Modeling Workflow](#modeling-workflow)
    - [Step 1: Run Initial Experiments with the "Do Nothing" Policy](#step-1-run-initial-experiments-with-the-do-nothing-policy)
    - [Step 2: Open Exploration: Uncertainty Analysis](#step-2-open-exploration-uncertainty-analysis)
    - [Step 3: Open Exploration: Scenario Discovery \& Selection](#step-3-open-exploration-scenario-discovery--selection)
        - [Step 3a: Scenario Discovery](#step-3a-scenario-discovery)
        - [Step 3b: Scenario Selection](#step-3b-scenario-selection)
    - [Step 4: Multi-Scenario, Multi-Objective Robust Policy Search](#step-4-multi-scenario-multi-objective-robust-policy-search)
        - [Step 4a: Generative Algorithm Policy Search](#step-4a-generative-algorithm-policy-search)
        - [Step 4b: Convergence Testing \& Initial Policy Filtering](#step-4b-convergence-testing--initial-policy-filtering)
        - [Step 4c: Robustness Testing](#step-4c-robustness-testing)
        - [Step 4d: Vulnerability Testing](#step-4d-vulnerability-testing)


## File Structure

An annotated directory `tree` is provided below, outlining the purpose of each category and highlighting specific individual files.

```
# Directories
├── data/
├── images/
├    ├── opt/
├    ├── prim/
├    └──  sobol/
├── results/
├    ├── op100/
├    ├── op150/
├    ├── op200/
├    ├── op150_1e4/
├    └──  op150_1e5/
# Base Model Files -- Untouched
├── __init__.py
├── dike_model_function.py
├── funs_dikes.py
├── funs_economy.py
├── funs_generate_network.py
├── funs_hydrostat.py
# Provided Workbench Files -- Unedited
├── problem_formulation.py
├── dike_model_simulation.py
# Experimentation & Analysis Files
# (in order of first use in modeling workflow)
├── dike_model_prim_analysis.ipynb
├── dike_model_visual_analysis.ipynb
├── dike_model_sobol_analysis.ipynb
├── scenario_selection.py
├── dike_model_scenario_selection.ipynb
├── dike_model_multi_scenario_MORDM.py
├── dike_model_multi_scenario_MORDM_reevaluation.py
├── dike_model_multi_scenario_MORDM.ipynb
# Other
├── requirements.txt
└── README.md

```

### Directories
* [archives/](archives) contains a history of the full run from the Optimization process, for reference and for use in checking whether the generative algorithm converged
* [data/](data) is a folder containing data used by the base model, without any modifications.
* [results/](results) contains all the output files generated during our analysis process, primarily consisting of CSV files or compressed tarballs of additional CSV files. There is a table in the [Modeling Workflow](#modeling-workflow) that describes these files and specifies their originating file. **NOTE**: Running our analysis will overwrite these files. It also contains a complete history of the run from the Optimization process, intended for reference and for verifying whether the generative algorithm converged.
  * [opt100/](opt100) contains all the optimizations that were simulated for 100.000 nfe's.
  * [opt150/](opt150) contains all the optimizations that were simulated for 150.000 nfe's.
  * [opt200/](opt200) contains all the optimizations that were simulated for 200.000 nfe's.
  * [opt50_1e4/](opt50_1e4) contains all the optimizations that were simulated for 50.000 nfe's with an epsilon value of 10.000.
  * [opt50_1e5/](opt50_1e5) contains all the optimizations that were simulated for 50.000 nfe's with an epsilon value of 100.000.
* [images/](images) contains all plots and diagrams generated by the code in our analysis files.
  * [opt/](opt) contains all the plots for the optimizations.
  * [prim/](prim) contains all the plots for the PRIM analysis.
  * [sobol/](sobol) contains all the plots for the Global Sensitivity Analysis (Sobol + Feature Scoring).  

### Model & Workbench Files
* The IJssel River model files were left untouched, as our client's needs did not require any modifications or extensions to the provided model.
* The provided workbench interface file, [problem_formulation.py](problem_formulation.py), was not modified

### Experimentation & Analysis Files (& Usage)
* These files make up the bulk of our work as modellers and analysts. They follow a linear pipeline process with an opportunity to plug final results back into the start of the process to iterate on scenario and policy discovery. 
* Unfortunately, we did not have the time to create one combined runfile that performs the entire process from start to finish.
* **These files are what to run to replicate our results. Their usage is described in [Modeling Workflow](#modeling-workflow), below.**

* These files constitute the majority of our work as modelers and analysts. They follow a linear process. The opportunity also exists to feed the final results back into the start of the process to iterate on scenario and policy discovery.
* Unfortunately, we did not have the time to create a single combined runfile that performs the entire process from start to finish.
* **These files should be run to replicate our results. Their usage is described in [Modeling Workflow](#modeling-workflow), below.**

### Other Files
* We include a requirements file that includes all the required and relevant packages to run the files included in this repository. 
* You are currently reading the README.
  
## Modeling Workflow
As mentioned, our modeling workflow is quite linear, with opportunities to feed results back to the starting point from multiple stages along the way. This section will describe the workflow and provide usage instructions for each file used at each step.



### Part 1: Uncertainty Analysis (Open Exploration)

**Files:** [dike_model_prim_analysis.ipynb](dike_model_prim_analysis.ipynb)

**Purpose & Output:** This component generates graphs that illustrate the significance of different uncertainties and levers on the overall performance and outcomes of the IJssel River model.

* Global Sensitivity Analysis: This involves creating multiple plots that visualize metrics from Sobol analysis, with a specific focus on S1, ST, and confidence intervals across various outcomes.
* Feature Scoring: This process generates a heatmap that quantifies the correlation between each uncertainty and the outcomes of interest.

**Instructions:** Open as Jupyter Notebooks and read results, or start a kernel and run from where results are loaded, until the bottom. **NOTE** Do not run the cell where experiments are perfomed as this may take quiet some time to execute.

**Required Input:** A valid experimental results file, likely the `sobol_results_problem2.tar.gz` or file 

### Step 2: Scenario Discovery & Selection (Open Exploration)


##### Step 2a: Visual Analysis & Scenario Discovery (Open Exploration)

**File:** **File:** [dike_model_prim_analysis.ipynb](dike_model_prim_analysis.ipynb) & [dike_model_visual_analysis.ipynb](dike_model_visual_analysis.ipynb)

**Purpose & Output:** 

* Generates graphs illustrating the parts of the outcome space and uncertainty space of the experiments conducted.
* Makes use of PRIM to identify a subspace within the model's uncertainty space that effectively represents the "region of concern." Scenarios falling within this subspace are likely to result in undesirable outcomes.

**Instructions:** Open as a Jupyter Notebook and read results, or start a kernel and run from top to bottom.

**Required Input:** A valid experimental (open exploration) results file, likely the `openexplor_problem1.tar.gz`, `openexplor_problem2.tar.gz` or `openexplor_problem3.tar.gz` file.

##### Part 2b: Scenario Selection

**File:** [dike_model_scenario_selection.ipynb](dike_model_scenario_selection.ipynb)

**Purpose & Output:** Samples 50.000 combinations of the filtered (post-PRIM) scenarios to identify a set of four scenarios that maximize diversity in terms of certain model outcomes.
* All sampled combinations include the "worst-case" scenario studied, which is the scenario that resulted in the greatest amount of deaths in Dike Rings 1 through 3 under the base case scenario, where nothing is done and no policies or mitigation measures are implemented.
* Diversity is evaluated based on the following outcomes: deaths to Dike Ring 1 through 3, combined damages to Dike Rings 1 & 2, and total damages. The goal is to find scenarios that cause disproportionately high amount of deaths to each of the regions.
* Outputs the selected set of scenarios to the file ([scenario_selection.tar.gz](results/scenario_selection.tar.gz)).

**Instructions:** Open as a Jupyter Notebook and read results, or start a kernel and run from top to bottom.

**Required Input:** A combined and PRIM-filtered experimental results table, such as `prim_problem3.tar.gz`.

### Part 3: Multi-Scenario, Multi-Objective Robust Decision-Making (Multi-scenario MORDM)


##### Step 3a: Generative Algorithm Policy Search

**File:** [optimization__seeded_fixed_scenario.py](optimization__seeded_fixed_scenario.py)

**Purpose & Output:** 
* For each scenario that was selected in Step 2b, runs a generative algorithm that discovers and assesses policies that optimize for the outcomes in our primary problem formulation. Repeats this process for 5 different seeds per scenario. 
* Saves output files `POLICY_SEARCH__results__*.csv`, `POLICY_SEARCH__convergence__*.csv` and archive files `POLICY_SEARCH__archive__*.csv`, in `output/` and `archives/`, respectively.
  * These files contain different parts of the results of this process:
    * The `results` files contain the policies found by the algorith alongside the model outcomes calculated for the scenario under which they were discovered.
    * The `convergence` files include a key indicator (`epsilon_progress`) of whether the generative algorithm converged to a final policy set.
    * The `archive` files include a history of how the algorithm generated and found new policies over time.

**Instructions:** 

```
python optimization__seeded_fixed_scenario.py`
```

**Required Input:** A table of scenarios in `scenario_selection.tar.gz`, which was generated in Step 2b.


##### Step 4b: Convergence Testing & Initial Policy Filtering

**File:** [Directed Search.ipynb](Directed%20Search.ipynb)

**Purpose & Output:** 
* Evaluates the optimization process carried out in the previous file, by plotting convergence metrics depicting how the generative algorithm moved towards optimization. These plots are saved in `img/`.
* Runs the model with the discovered policies under the scenarios in which they were discovered, to calculate a wider set of model outcomes. 
* Filters the discovered policies according to a set of constraints defined to match the Clients' goals.
* Selects a set of 50 such policies that are internally diverse across the set of levers they activate, and saves these to a file
* Saves output files `output/policies__constraints_filtered.csv` and `output/policies__constraints_filtered__diverse_50.csv` that include the policy sets being considered in future steps. We include this in our output so that the Client can peruse other possible options that our algorithmic process produced.

<span style="color:red">**WARNING**:</span> For some reason (we suspect it might be due to how seeds are handled in the IJssel River model itself?), the policy sets produced in this file change on every run. The larger, filtered set is with >95% the same, with just a few policies that are on the edge of passing our constraints sometimes making it and sometimes not. The "diverse 50" set thus varies quite highly on each run due to these small changes in the input set to its diversity serach.

**Instructions:** 
Open as a Jupyter Notebook and run from top to bottom. **Calculating the convergence metrics takes 5-10 minutes, so can be commented out (also comment out the following cell that plots the metrics).**

**Required Input:** A set of results from the optimization process, above, and the table of scenarios in `selected_scenarios.csv` for which that optimization process was run.

##### Step 4c: Robustness Testing

**Files:** [`run_experiments.py`](run_experiments.py) & [`Policy Robustness.ipynb`](`Policy%20Robustness.ipynb`)

**Purpose & Output:** 
* Perform EMA workbench experiments on the filtered policy set using 1000 random scenarios, and save the results to the file `robustness_results__1000_scenarios.tar.gz`. See Step 1 for a general description `run_experiments.py`.
* Calculate a set of robustness metrics based on the experiment results from the previous substep.
* Generates and saves graphs comparing the set of 50 policies according to these metrics.
* A mix of automated rules and qualitative analyst analysis are used to further downselect to 5 or fewer policies, which are saved in `outputs/policies__final_set.csv`.

**Instructions:**

```
python run_experiments.py mode=robustness num_scenarios=1000
```

Then open `Policy Robustness.ipynb` as a Jupyter Notebook and run from top to bottom or read the in-line analysis.

**Required Input:** 

* In this mode, `run_experiments.py` requires a file called `output/policies__constraints_filtered__diverse_set_50.csv`, which is generated in the previous substep.
* If you input a `num_scenarios` other than 1000, you will have to edit the input filename in the Robustness Notebook. This Notebook also requires a CSV from one of the previous substeps where a set of policies were captured, from which to read, filter, and re-write the final policy set.

<span style="color:red">**WARNING**:</span> Due to the issue described in the previous substep, the `robustness_results__1000_scenarios.tar.gz` being submitted was generated based on a different input set than the diverse policy set CSV being submitted. We ran out of time to re-run this process and re-analyse the results in the following step, but trust that these experiments were run for a real set of policies generated by the previous substep.


##### Step 4d: Vulnerability Testing

**Files:** [`run_experiments.py`](run_experiments.py) & [`Policy Vulnerability.ipynb`](`Policy%20Vulnerability.ipynb`)

**Purpose & Output:**
* Run a process much like Scenario Discovery on the originally-marked scenarios of concern (those that fall within the original PRIM box) and the discovered candidate policy solutions.
* Discover a bounding box of a subspace of "great concern," by running PRIM on the results of this new round of experimentation.
* Filter down to a subset of scenario-policy pairs that are of great concern.
* Interrogate meaningful advice that can be derived from understanding these scenarios of concern for the proposed policies.

**Instructions:**

```
python run_experiments.py mode=vulnerability
```

Then open `Policy Vulnerability.ipynb` as a Jupyter Notebook and run from top to bottom or read the in-line analysis.

**Required Input:** 

* In this mode, `run_experiments.py` requires a file called `output/policies__final_set.csv`, which is generated in the previous substep.
