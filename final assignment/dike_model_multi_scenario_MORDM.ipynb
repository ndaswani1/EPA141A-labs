{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "from Bartholomew & Kwakkel (2020):\n",
    "\n",
    "\n",
    "2.2. Multi-scenario many-objective robust decision making\n",
    "Multi-scenario MORDM (Watson and Kasprzyk, 2017) is a further\n",
    "refinement of MORDM. The main contribution of MORDM was the\n",
    "use of a MOEA for finding a set of promising candidate solutions\n",
    "which together capture the key trade-offs amongst competing objec\u0002tives. However, this search uses a single reference scenario, and it is\n",
    "unlikely that solutions that are optimal in a given scenario are also\n",
    "optimally robust. Multi-scenario MORDM (Fig. 1(c)) addresses this by\n",
    "performing a search for candidate strategies for several different refer\u0002ence scenarios. The additional scenarios for which search is performed\n",
    "are selected from regions in the deep uncertainty space where candidate\n",
    "solutions found for the first reference scenario are failing to meet their\n",
    "objectives. So, one performs the four MORDM steps, and based on\n",
    "the insights from scenario discovery, additional scenarios are selected\n",
    "for which search is also performed. The goal of this is to build a\n",
    "more diverse set of policy alternatives which are Pareto optimal under\n",
    "different scenarios.\n",
    "The selection of scenarios after the first MORDM iteration is a crit\u0002ical step in multi-scenario MORDM (Eker and Kwakkel, 2018). Watson\n",
    "and Kasprzyk (2017) suggest picking scenarios based on the scenario\n",
    "discovery results. The number of scenarios to select is left to the\n",
    "analyst. Clearly, if the number of scenarios for which a search is\n",
    "conducted increases, the chance of finding solutions that are robust\n",
    "during the re-evaluation also increases. However, this comes at a sub\u0002stantial computational cost. To assist in balancing comprehensiveness\n",
    "and computational cost, while making scenario selection transparent\n",
    "and reproducible, Eker and Kwakkel (2018) present an approach for\n",
    "finding the most policy relevant and maximally diverse set of scenarios.\n",
    "Policy relevance is defined as scenarios that lead to poor outcomes and\n",
    "the diversity criterion is based on Carlsen et al. (2016)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A. Multi-Scenario Many-Objective Robust Decision Making"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A.1. Problem formulation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A.2. Scenario selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A.3. Generating candidate solutions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A.4. Tradeoff analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Signal-to-Noise**\\\n",
    "To quantify the robustness we can use multiple robustness metrices. The first one that we use is the **signal-to-noise ratio**. This is the mean of a dataset divided by the standard deviation. If we want to minimize the outcomes, a low mean and a low standard deviation is preferred."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def signal_to_noise(data, direction):\n",
    "    \"Calculate the signal-to-noise ratio of a dataset with outcome directions (minimize or maximize)\"\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    if direction==ScalarOutcome.MAXIMIZE:\n",
    "        return mean/std\n",
    "    else:\n",
    "        return mean*std"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Empty dictionary to collect the ratio scores of every policy:\n",
    "overall_scores = {}\n",
    "\n",
    "# Loop over all policies and outcomes respectively\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "    \n",
    "    logical = experiments['policy']==policy\n",
    "    \n",
    "    for outcome in model.outcomes:\n",
    "        value  = outcomes[outcome.name][logical]\n",
    "        sn_ratio = s_to_n(value, outcome.kind)\n",
    "        scores[outcome.name] = sn_ratio\n",
    "    overall_scores[policy] = scores\n",
    "    \n",
    "# Create dataframe for visualisation purposes:\n",
    "scores = pd.DataFrame.from_dict(overall_scores).T\n",
    "scores"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualise tradeoffs on a parallel plot:\n",
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "data = scores\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['A.1 Total Costs', 'A.1_Expected Number of Deaths', 'A.2 Total Costs', 'A.2_Expected Number of Deaths', 'A.3 Total Costs', 'A.3_Expected Number of Deaths','RfR Total Costs', 'Expected Evacuation Costs']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis('A.1 Total Costs')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Minimax regret**\n",
    "Another robustness metric that we use for our analysis is the **minimax regret** metric. This metric computes the regret for each option, then takes the ones with the maximum regret (worst-case), and then chooses the options that minimize this maximum regret. We define regret as the difference between the performance of a policy in a scenario and the performance of the best possible result in that scenario or the reference policy. The province of Gelderland is in favour of policy options with low maximum regret values, because the province is responsible for the safety of her region and pays the policies with government money. So, the province has a high level of risk aversion and therefore this robustness metric is suitable for our analysis.\n",
    "\n",
    "Source: https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2017EF000649"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
