{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "from Bartholomew & Kwakkel (2020):\n",
    "\n",
    "\n",
    "2.2. Multi-scenario many-objective robust decision making\n",
    "Multi-scenario MORDM (Watson and Kasprzyk, 2017) is a further\n",
    "refinement of MORDM. The main contribution of MORDM was the\n",
    "use of a MOEA for finding a set of promising candidate solutions\n",
    "which together capture the key trade-offs amongst competing objec\u0002tives. However, this search uses a single reference scenario, and it is\n",
    "unlikely that solutions that are optimal in a given scenario are also\n",
    "optimally robust. Multi-scenario MORDM (Fig. 1(c)) addresses this by\n",
    "performing a search for candidate strategies for several different refer\u0002ence scenarios. The additional scenarios for which search is performed\n",
    "are selected from regions in the deep uncertainty space where candidate\n",
    "solutions found for the first reference scenario are failing to meet their\n",
    "objectives. So, one performs the four MORDM steps, and based on\n",
    "the insights from scenario discovery, additional scenarios are selected\n",
    "for which search is also performed. The goal of this is to build a\n",
    "more diverse set of policy alternatives which are Pareto optimal under\n",
    "different scenarios.\n",
    "The selection of scenarios after the first MORDM iteration is a crit\u0002ical step in multi-scenario MORDM (Eker and Kwakkel, 2018). Watson\n",
    "and Kasprzyk (2017) suggest picking scenarios based on the scenario\n",
    "discovery results. The number of scenarios to select is left to the\n",
    "analyst. Clearly, if the number of scenarios for which a search is\n",
    "conducted increases, the chance of finding solutions that are robust\n",
    "during the re-evaluation also increases. However, this comes at a sub\u0002stantial computational cost. To assist in balancing comprehensiveness\n",
    "and computational cost, while making scenario selection transparent\n",
    "and reproducible, Eker and Kwakkel (2018) present an approach for\n",
    "finding the most policy relevant and maximally diverse set of scenarios.\n",
    "Policy relevance is defined as scenarios that lead to poor outcomes and\n",
    "the diversity criterion is based on Carlsen et al. (2016)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# A. Multi-Scenario Many-Objective Robust Decision Making"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A.1. Problem formulation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A.2. Scenario selection"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is desirable to search for candidate solutions under a variety of scenarios, so that robustness of potential solutions is increased (Eker and Kwakkel, 2018).\n",
    "\n",
    "In this study, our purpose for scenario selection is to find sce\u0002narios that potentially lead to solutions that are more robust when\n",
    "the search phase of MORDM is conducted for each of these sce\u0002narios. Therefore, among the possible scenario selection criteria\n",
    "listed above, we consider policy relevance and diversity as the\n",
    "relevant ones. Policy-relevance is a problem-specific concept that\n",
    "should reflect the decision-making concerns and preferences. In\n",
    "this study, we choose to define policy-relevance with respect to\n",
    "undesirable scenario conditions specified by the median values of\n",
    "the scenario space. Dividing the scenario space at the median\n",
    "values focuses the diversity analysis on a smaller number of policy\u0002relevant scenarios regardless of their distribution. However, we do\n",
    "not propose this as a general way to define policy relevance. For the\n",
    "diversity criterion, we propose to use the specific diversity\n",
    "maximization method introduced by Carlsen et al. (2016b).\n",
    "\n",
    "Following Eker and Kwakkel (2018), scenario selection is conducted in three steps, namely\n",
    "- 1. Scenario generation;\n",
    "- 2. Filtering of policy-relevant scenarios;\n",
    "- 3. Selecting maximally diverse scenarios."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A.2.1. Scenario generation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this step, N scenarios will be generated using randomly sampled values of deep uncertainties and decision levers (Eker and Kwakkel, 2018) ."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A.2.2. Filtering of policy-relevant scenarios"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Due to computational restrictions, only a limited amount of scenarios is used. In this step, scenarios are filtered based on their policy relevance.\n",
    "\n",
    "In the next step, scenarios will be selected using an indicator for scenario diversity.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### A.2.3. Selecting maximally diverse scenarios"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is desirable to select maximally diverse scenarios, so that ... ? In this step, maximally diverse scenarios will be selected following the approach of Carlsen et al. (2016)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Normalizing all outcomes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "#\n",
    "# experiments_of_interest = experiments.loc[y]\n",
    "# outcomes_df = pd.DataFrame({k:v[y] for k,v in outcomes.items()})\n",
    "#\n",
    "# # normalize outcomes on unit interval to ensure equal weighting of outcomes\n",
    "# x = outcomes_df.values\n",
    "# min_max_scaler = preprocessing.MinMaxScaler()\n",
    "# x_scaled = min_max_scaler.fit_transform(x)\n",
    "# normalized_outcomes = pd.DataFrame(x_scaled, columns=outcomes_df.columns)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating all possible permutations of 4 scenarios"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import itertools\n",
    "#\n",
    "# n_scen = experiments.loc[y].shape[0]\n",
    "# indices = range(n_scen)\n",
    "# set_size = 4\n",
    "# n_scen\n",
    "# combinations = itertools.combinations(indices, set_size)\n",
    "# combinations = list(combinations)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Calculating diversity of each of the combinations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #The following code snippet is adopted from assignment 10 model answers (Kwakkel, 2024)\n",
    "#\n",
    "# from concurrent.futures import ProcessPoolExecutor\n",
    "# import os\n",
    "# import functools\n",
    "# from scipy.spatial.distance import pdist, squareform\n",
    "#\n",
    "# # the relevant code is in a .py file to esnure parallization works within the notebook\n",
    "# from assignment_9_scenario_selection import find_maxdiverse_scenarios\n",
    "#\n",
    "# # calculate the pairwise distances between the normalized outcomes\n",
    "# distances = squareform(pdist(normalized_outcomes.values))\n",
    "#\n",
    "# cores = os.cpu_count()\n",
    "# partial_function = functools.partial(find_maxdiverse_scenarios, distances)\n",
    "#\n",
    "# # setup the pool of workers and split the calculations over the set of cores\n",
    "# with ProcessPoolExecutor(max_workers=cores) as executor:\n",
    "#     worker_data = np.array_split(sampled_combinations, cores)\n",
    "#     results = [e for e in executor.map(partial_function, worker_data)]\n",
    "#     results = list(itertools.chain.from_iterable(results))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Check most diverse results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #The following code snippet is adopted from assignment 10 model answers (Kwakkel, 2024)\n",
    "#\n",
    "# results.sort(key=lambda entry:entry[0], reverse=True)\n",
    "# most_diverse = results[0]\n",
    "# most_diverse"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create scenarios using most diverse scenarios"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# #The following code snippet is adopted from assignment 10 model answers (Kwakkel, 2024)\n",
    "#\n",
    "# from ema_workbench import Scenario\n",
    "#\n",
    "# selected = experiments.loc[most_diverse[1], ['b', 'delta', 'mean', 'q', 'stdev']]\n",
    "# scenarios = [Scenario(f\"{index}\", **row) for index, row in selected.iterrows()]\n",
    "#\n",
    "# for scenario in scenarios:\n",
    "#     print(scenario)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A.3. Generating candidate solutions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Following the scenario selection of the previous step, in this step the Multi-Objective Evolutionary Algorithm (MOEA) epsilon-NSGAII algorithm (Kollat and Reed, 2006)  is used to generate candidate solutions, following the approach of Eker and Kwakkel (2018). + explain how epsilon-NSGAII algorithm works + explain benefits of epsilon-NSGAII algorithm for this case"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Creating a function to optimize under a certain scenario"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "#The following code snippet is adopted from assignment 10 model answers (Kwakkel, 2024)\n",
    "\n",
    "# from ema_workbench import MultiprocessingEvaluator, ema_logging\n",
    "# from ema_workbench.em_framework.evaluators import BaseEvaluator\n",
    "#\n",
    "# from ema_workbench.em_framework.optimization import (ArchiveLogger,\n",
    "#                                                      EpsilonProgress,\n",
    "#                                                      to_problem, epsilon_nondominated)\n",
    "#\n",
    "# ema_logging.log_to_stderr(ema_logging.INFO)\n",
    "#\n",
    "# def optimize(scenario, nfe, model, epsilons):\n",
    "#     results = []\n",
    "#     convergences = []\n",
    "#     problem = to_problem(model, searchover=\"levers\")\n",
    "#\n",
    "#     with MultiprocessingEvaluator(model) as evaluator:\n",
    "#         for i in range(5):\n",
    "#             convergence_metrics = [\n",
    "#                 ArchiveLogger(\n",
    "#                     \"./archives\",\n",
    "#                     [l.name for l in model.levers],\n",
    "#                     [o.name for o in model.outcomes],\n",
    "#                     base_filename=f\"assignemnt_9_{scenario.name}_seed_{i}.tar.gz\",\n",
    "#                 ),\n",
    "#                 EpsilonProgress(),\n",
    "#             ]\n",
    "#\n",
    "#             result, convergence = evaluator.optimize(nfe=nfe, searchover='levers',\n",
    "#                                          convergence=convergence_metrics,\n",
    "#                                          epsilons=epsilons,\n",
    "#                                          reference=scenario)\n",
    "#\n",
    "#             results.append(result)\n",
    "#             convergences.append(convergence)\n",
    "#\n",
    "#     # merge the results using a non-dominated sort\n",
    "#     reference_set = epsilon_nondominated(results, epsilons, problem)\n",
    "#\n",
    "#     return reference_set, convergences\n",
    "#\n",
    "#\n",
    "# results = []\n",
    "# for scenario in scenarios:\n",
    "#     epsilons = [0.05,]*len(model.outcomes)\n",
    "#\n",
    "#     # note that 100000 nfe is again rather low to ensure proper convergence\n",
    "#     results.append(optimize(scenario, 1e5, model, epsilons))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Scenario 1 (reference scenario)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Scenario 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Scenario 3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Scenario 4"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Assess convergence"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Combining the pareto set of solutions found for each scenario"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### (If you have a very large number of policies, you can choose to down sample your policies in some reasoned way (e.g., picking min and max on each objective, slicing across the pareto front with a particular step size). As a rule of thumb, try to limit the set of policies to at most 50.)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from ema_workbench import Policy\n",
    "#\n",
    "# policies = []\n",
    "# for i, (result, _) in enumerate(results):\n",
    "#     result = result.iloc[:, 0:5]\n",
    "#     for j, row in result.iterrows():\n",
    "#         policy = Policy(f'scenario {i} option {j}', **row.to_dict())\n",
    "#         policies.append(policy)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Re-evaluate under deep uncertainty\n",
    "Re-evaluate the combined set of solutions over 1000 scenarios sampled using LHS"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# with MultiprocessingEvaluator(model) as evaluator:\n",
    "#     reeevaluation_results = evaluator.perform_experiments(1000, policies=policies)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### A.4. Tradeoff analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# experiments, outcomes = reeevaluation_results\n",
    "#...."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Signal-to-Noise**\\\n",
    "To quantify the robustness we can use multiple robustness metrices. The first one that we use is the **signal-to-noise ratio**. This is the mean of a dataset divided by the standard deviation. If we want to minimize the outcomes, a low mean and a low standard deviation is preferred."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def signal_to_noise(data, direction):\n",
    "    \"Calculate the signal-to-noise ratio of a dataset with outcome directions (minimize or maximize)\"\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    \n",
    "    if direction==ScalarOutcome.MAXIMIZE:\n",
    "        return mean/std\n",
    "    else:\n",
    "        return mean*std"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Empty dictionary to collect the ratio scores of every policy:\n",
    "overall_scores = {}\n",
    "\n",
    "# Loop over all policies and outcomes respectively\n",
    "for policy in np.unique(experiments['policy']):\n",
    "    scores = {}\n",
    "    \n",
    "    logical = experiments['policy']==policy\n",
    "    \n",
    "    for outcome in model.outcomes:\n",
    "        value  = outcomes[outcome.name][logical]\n",
    "        sn_ratio = s_to_n(value, outcome.kind)\n",
    "        scores[outcome.name] = sn_ratio\n",
    "    overall_scores[policy] = scores\n",
    "    \n",
    "# Create dataframe for visualisation purposes:\n",
    "scores = pd.DataFrame.from_dict(overall_scores).T\n",
    "scores"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualise tradeoffs on a parallel plot:\n",
    "from ema_workbench.analysis import parcoords\n",
    "\n",
    "data = scores\n",
    "limits = parcoords.get_limits(data)\n",
    "limits.loc[0, ['A.1 Total Costs', 'A.1_Expected Number of Deaths', 'A.2 Total Costs', 'A.2_Expected Number of Deaths', 'A.3 Total Costs', 'A.3_Expected Number of Deaths','RfR Total Costs', 'Expected Evacuation Costs']] = 0\n",
    "\n",
    "paraxes = parcoords.ParallelAxes(limits)\n",
    "paraxes.plot(data)\n",
    "paraxes.invert_axis('A.1 Total Costs')\n",
    "plt.show()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Minimax regret**\n",
    "Another robustness metric that we use for our analysis is the **minimax regret** metric. This metric computes the regret for each option, then takes the ones with the maximum regret (worst-case), and then chooses the options that minimize this maximum regret. We define regret as the difference between the performance of a policy in a scenario and the performance of the best possible result in that scenario or the reference policy. The province of Gelderland is in favour of policy options with low maximum regret values, because the province is responsible for the safety of her region and pays the policies with government money. So, the province has a high level of risk aversion and therefore this robustness metric is suitable for our analysis.\n",
    "\n",
    "Source: https://agupubs.onlinelibrary.wiley.com/doi/epdf/10.1002/2017EF000649"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
